{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf4fcc6e-89aa-48bc-bae8-68b66d6e19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
    "                          BitsAndBytesConfig, TrainingArguments)\n",
    "from trl import SFTTrainer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2db7c3a8-0a69-4ab7-b5e6-18ef0412a0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4b6f2ce4be46e2812a62870aabf680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output'],\n",
      "    num_rows: 3077\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbdece9e4fa4aafb1a5a01940f83767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"../models/\"\n",
    "\n",
    "data_path = \"../problem_data/train.jsonl\"\n",
    "result_adapter = \"../models/llama3-ko-alwayssaewoo_problem-adapter\"\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "print(dataset)\n",
    "\n",
    "def combine(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    output_text = example[\"output\"]\n",
    "    example[\"text\"] = f\"<s>[INST] {instruction} {input_text} [/INST] {output_text}</s>\"\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(combine)\n",
    "# í† í¬ë‚˜ì´ì €\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a85e662-1efd-46a5-ab95-5a511ef432f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¡œë“œ Qunatization\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4e9bddb-bd14-4e6f-bca8-7e045ab3f3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3784a65e4d044872b83122e0be6f83f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ë¡œë“œ\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = quant_config\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abd6d252-3f5b-45e2-893a-d4f3d785d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ì„¸íŒ…\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1,\n",
    "    r = 16,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a16fc6-2559-4aca-85b8-86aaf6ca8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ arg\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./arg\", # ì²´í¬í¬ì¸íŠ¸, ë¡œê·¸ ì €ì¥ ê²½ë¡œ\n",
    "    save_total_limit = 2, # ./arg ì €ì¥ ì²´í¬í¬ì¸íŠ¸ ìµœëŒ€ ê°œìˆ˜\n",
    "        logging_steps = 10, # ë¡œê·¸ ì¶œë ¥ ê°„ê²©\n",
    "        num_train_epochs = 3, # ì—í­\n",
    "        per_device_train_batch_size = 4, # GPUë‹¹ ë°°ì¹˜ í¬ê¸°\n",
    "        gradient_accumulation_steps = 8, # gradient accumulation (ë¯¸ë‹ˆ ë°°ì¹˜ í›„ ì—…ë°ì´íŠ¸)\n",
    "        learning_rate = 2e-4, # í•™ìŠµë¥ \n",
    "        save_steps = 50, # ëª¨ë¸ ì €ì¥ ì—…ë°ì´íŠ¸ ê°„ê²©\n",
    "        fp16 = True, # fp16 ì„¤ì •\n",
    "        eval_strategy = \"no\" # ëª¨ë¸ í‰ê°€ no\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d93b7-399c-49f6-ae12-6809195f709f",
   "metadata": {},
   "source": [
    "---\n",
    "í…ŒìŠ¤íŠ¸ìš©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d0b7432-7293-4ab5-a655-3faba692ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f975a7681746b48859cf7750cedc80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    }
   ],
   "source": [
    "LoRA_tokenizer = AutoTokenizer.from_pretrained(result_adapter, use_fast=False)\n",
    "LoRA_model = AutoModelForCausalLM.from_pretrained(\n",
    "     model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "696b413e-d9b5-483c-a43e-9128e04bc4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"../data/test.jsonl\"\n",
    "test_dataset = load_dataset(\"json\", data_files=test_data_path, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "838fcc4b-6645-4640-87ca-d833ca276b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=LoRA_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=test_dataset,  \n",
    "    peft_config=lora_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a7f6bba-ae05-4e2b-b7ac-911d507a943f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4994373321533203, 'eval_model_preparation_time': 0.0024, 'eval_runtime': 8.4278, 'eval_samples_per_second': 12.221, 'eval_steps_per_second': 1.543, 'eval_entropy': 2.2511578523195706, 'eval_num_tokens': 0.0, 'eval_mean_token_accuracy': 0.510786411853937}\n"
     ]
    }
   ],
   "source": [
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8467db-8e08-461a-8084-06b44f361a25",
   "metadata": {},
   "source": [
    "# í•™ìŠµìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20821df2-00db-4a16-aa17-abf39f80efd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dfa5736cd24d1c9af7ab08d3960a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/3077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0a2b5b352c4d1d8d47b0ee5c495200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ccd7a50f904ed5a396a4c45103860d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/3077 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80d8a474-ace9-4963-9596-ab30ef916397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128001}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='291' max='291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [291/291 19:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.595600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.849600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.773400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.685300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.613300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.595700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.540300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.465800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.444700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.438700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.465800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.457500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.407300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.421100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.397200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.384500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.385600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(result_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cdf9a87-3d9b-47e1-a66d-9a7c51f62595",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_adapter = \"../models/llama3-ko-alwayssaewoo_problem-adapter\"\n",
    "trainer.save_model(result_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0009a840-bb06-4cb6-8c30-39fd38946b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51ef0ee7-1ece-4b20-9559-e4e2daa4c842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ì…ë ¥ ===\n",
      "ê°ì²´ì§€í–¥ ì„¤ê³„ì— ëŒ€í•´ ì•Œë ¤ì¤˜\n",
      "=== ëª¨ë¸ ì¶œë ¥ ===\n",
      "ê°ì²´ì§€í–¥ ì„¤ê³„ì— ëŒ€í•´ ì•Œë ¤ì¤˜ë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”. ê°ì²´ì§€í–¥ ì„¤ê³„ì˜ í•µì‹¬ ê°œë…ê³¼ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”. \n",
      "\n",
      "ê°ì²´ì§€í–¥ ì„¤ê³„ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. ê°ì²´ì§€í–¥ ì„¤ê³„ì˜ í•µì‹¬ ê°œë…ê³¼ ì¥ì ì„ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ê°ì²´ì§€í–¥ ì„¤ê³„ì˜ í•µì‹¬ ê°œë…\n",
      "\n",
      "1. **ê°ì²´(Object)**: ê°ì²´ëŠ” ì‹¤ì„¸ê³„ì˜ ì‚¬ë¬¼ì´ë‚˜ ê°œë…ì„ ì½”ë“œë¡œ í‘œí˜„í•œ ê²ƒì…ë‹ˆë‹¤. ê°ì²´ëŠ” ì†ì„±(ë°ì´í„°)ê³¼ ë©”ì„œë“œ(ê¸°ëŠ¥)ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **í´ë˜ìŠ¤(Class)**: í´ë˜ìŠ¤ëŠ” ê°ì²´ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ì²­ì‚¬ì§„ì…ë‹ˆë‹¤. í´ë˜ìŠ¤ëŠ” ê°ì²´ì˜ ì†ì„±ê³¼ ë©”ì„œë“œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
      "\n",
      "3. **ìƒì†(Inheritance)**: ìƒì†ì€ ê¸°ì¡´ í´ë˜ìŠ¤ì˜ ì†ì„±ê³¼ ë©”ì„œë“œë¥¼ ë¬¼ë ¤ë°›ì•„ ìƒˆë¡œìš´ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì½”ë“œì˜ ì¬ì‚¬ìš©ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. **ë‹¤í˜•ì„±(Polymorphism)**: ë‹¤í˜•ì„±ì€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë‚˜ ë©”ì„œë“œë¥¼ í†µí•´ ë‹¤ì–‘í•œ ê°ì²´ë¥¼ ë‹¤ë£° ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ì´ëŠ” ì½”ë“œì˜ ìœ ì—°ì„±ê³¼ í™•ì¥ì„±ì„ ë†’ì…ë‹ˆë‹¤.\n",
      "\n",
      "5. **ìº¡ìŠí™”(Encapsulation)**: ìº¡ìŠí™”ëŠ” ê°ì²´ì˜ ì†ì„±ì„ ì™¸ë¶€ë¡œë¶€í„° ìˆ¨ê¸°ê³ , í•„ìš”í•œ ê²½ìš°ì—ë§Œ ê³µê°œí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„°ì˜ ë³´í˜¸ì™€ ì½”ë“œì˜ ìœ ì§€ë³´ìˆ˜ë¥¼ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
      "\n",
      "6. **Composition**: êµ¬ì„± ê´€ê³„ëŠ” í•˜ë‚˜ì˜ ê°ì²´ê°€ ë‹¤ë¥¸ ê°ì²´ì˜ ì§‘í•©ì¸ ê²½ìš°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ë³µì¡í•œ ê°ì²´ë¥¼ ë‹¨ìˆœí™”í•˜ê³ , ì½”ë“œì˜ ì¬ì‚¬ìš©ì„±ì„ ë†’ì…ë‹ˆë‹¤.\n",
      "\n",
      "### ê°ì²´ì§€í–¥ ì„¤ê³„ì˜ ì¥ì \n",
      "\n",
      "1. **ëª¨ë“ˆí™”**: ê°ì²´ì§€í–¥ ì„¤ê³„ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ì‘ì€ ëª¨ë“ˆë¡œ ë‚˜ëˆ„ì–´ ê´€ë¦¬í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ê° ëª¨ë“ˆì€ ë…ë¦½ì ìœ¼ë¡œ ê°œë°œë˜ê³  í…ŒìŠ¤íŠ¸ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ì¬ì‚¬ìš©ì„±**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¯¸ ì‘ì„±ëœ í´ë˜ìŠ¤ë‚˜ ë©”ì„œë“œë¥¼ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "3. **ìœ ì§€ë³´ìˆ˜ì„±**: ì½”ë“œì˜ ìœ ì§€ë³´ìˆ˜ê°€ ìš©ì´í•©ë‹ˆë‹¤. ê°ì²´ì˜ ë³€ê²½ì€ í•´ë‹¹ ê°ì²´ë§Œì„ ë³€ê²½í•˜ë©´ ë˜ë¯€ë¡œ, ì „ì²´ ì‹œìŠ¤í…œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ ì ìŠµë‹ˆë‹¤.\n",
      "\n",
      "4. **í™•ì¥ì„±**: ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ê±°ë‚˜ ê¸°ì¡´ ê¸°ëŠ¥ì„ ìˆ˜ì •í•  ë•Œ ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤í˜•ì„±ê³¼ ìƒì†ì„ í†µí•´ ì‰½ê²Œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "5. **ì ì‘ë ¥**: ê°ì²´ì§€í–¥ ì„¤ê³„ëŠ” ë³µì¡í•œ ë¬¸ì œë¥¼ ë‹¨ìˆœí™”í•˜ê³ , ë‹¤ì–‘í•œ ìƒí™©ì— ì ì‘í•  ìˆ˜ ìˆëŠ” ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
      "\n",
      "6. **í…ŒìŠ¤íŠ¸ ìš©ì´ì„±**: ê°ì²´ì§€í–¥ ì„¤ê³„ëŠ” í…ŒìŠ¤íŠ¸ê°€ ìš©ì´í•©ë‹ˆë‹¤. ê° ê°ì²´ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìœ¼ë©°, í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  ì‹¤í–‰í•˜ëŠ” ê²ƒì´ ê°„ë‹¨í•©ë‹ˆë‹¤.\n",
      "\n",
      "### ê²°ë¡ \n",
      "\n",
      "ê°ì²´ì§€í–¥ ì„¤ê³„ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤. í´ë˜ìŠ¤, ìƒì†, ë‹¤í˜•ì„±, ìº¡ìŠí™”, êµ¬ì„± ë“± ë‹¤ì–‘í•œ ê°œë…ì„ í†µí•´ ì½”ë“œì˜ ì¬ì‚¬ìš©ì„±, ìœ ì§€ë³´ìˆ˜ì„±, í™•ì¥ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¥ì ì„ í†µí•´ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œì˜ íš¨ìœ¨ì„±ì„ ë†’ì´ê³ , ë” ë‚˜ì€ ì†Œí”„íŠ¸ì›¨ì–´ë¥¼ ê°œë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°ì²´ì§€í–¥ ì„¤ê³„ë¥¼ ì˜ ì´í•´í•˜ê³  í™œìš©í•˜ëŠ” ê²ƒì€ ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œìì—ê²Œ í•„ìˆ˜ì ì¸ ê¸°ìˆ ì…ë‹ˆë‹¤. \n",
      "\n",
      "ê°ì²´ì§€í–¥ ì„¤ê³„ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ˜Š......................................................................................................................................................................................................................................\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=LoRA_model,\n",
    "    tokenizer=LoRA_tokenizer,\n",
    ")\n",
    "\n",
    "for i in range(1):\n",
    "    prompt = \"ê°ì²´ì§€í–¥ ì„¤ê³„ì— ëŒ€í•´ ì•Œë ¤ì¤˜\"#test_dataset[i][\"instruction\"]\n",
    "    print(\"=== ì…ë ¥ ===\")\n",
    "    print(prompt)\n",
    "    print(\"=== ëª¨ë¸ ì¶œë ¥ ===\")\n",
    "    print(gen(prompt, max_new_tokens=1000)[0][\"generated_text\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cbe45-000b-4473-ac6a-73b8df2b77ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
